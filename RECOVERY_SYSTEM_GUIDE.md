# NexusLang v2 Recovery & Analytics System
## Complete Guide

**Created**: November 12, 2025  
**Version**: 2.0.0-beta  
**Status**: ‚úÖ Production Ready

---

## üéØ Overview

This system eliminates the pain of RunPod pod restarts. Instead of 20+ minutes of manual reinstallation, you now have:

- **< 2 minute recovery** time
- **Automatic database backup/restore**
- **Comprehensive analytics** tracking
- **Zero-configuration deployment**

---

## üöÄ Quick Start (RunPod Deployment)

### One-Command Deployment

```bash
# On your RunPod terminal
cd /workspace/nexuslang-v2
python recovery.py
```

That's it! Your entire platform will be deployed in under 2 minutes.

### What It Does

1. ‚úÖ Auto-detects Docker availability
2. ‚úÖ Installs PostgreSQL, Redis, Python packages
3. ‚úÖ Fixes all import issues automatically
4. ‚úÖ Restores latest database backup
5. ‚úÖ Runs comprehensive health checks
6. ‚úÖ Starts the server
7. ‚úÖ Displays access URLs

---

## üì¶ Recovery System Components

### 1. Docker-Based Recovery (Fastest)

**File**: [`v2/backend/Dockerfile.production`](v2/backend/Dockerfile.production)

Pre-built Docker image with:
- All Python dependencies pre-installed
- Import fixes baked in
- PostgreSQL client included
- Health checks built-in

**Usage**:
```bash
# Build image (one time)
cd v2/backend
bash build_docker.sh

# Deploy on RunPod
docker run -p 8000:8000 -v /workspace:/workspace galion/nexuslang-v2:latest
```

### 2. Manual Recovery (Fallback)

**File**: [`v2/backend/recovery.py`](v2/backend/recovery.py)

Automatically:
- Installs system packages
- Sets up PostgreSQL and Redis
- Installs Python dependencies
- Fixes import issues
- Restores database
- Runs health checks

**Usage**:
```bash
python recovery.py              # Full recovery
python recovery.py --skip-backup  # Skip database restore
python recovery.py --manual     # Force manual (no Docker)
```

---

## üóÑÔ∏è Database Backup System

### Automated Backups

**File**: [`v2/backend/scripts/backup_database.py`](v2/backend/scripts/backup_database.py)

Features:
- Compressed gzip backups
- 7-day retention policy
- Optional S3 upload
- Metadata tracking

**Usage**:
```bash
# Create backup
python scripts/backup_database.py

# Upload to S3
python scripts/backup_database.py --upload-s3

# Custom retention
python scripts/backup_database.py --retention-days 14

# List backups
python scripts/backup_database.py --list
```

### Scheduled Backups

**File**: [`v2/backend/scripts/backup_scheduler.py`](v2/backend/scripts/backup_scheduler.py)

Run automated backups on schedule:

```bash
# Daily backups at 3 AM UTC
python scripts/backup_scheduler.py

# Every 6 hours
python scripts/backup_scheduler.py --interval 6

# Run once and exit
python scripts/backup_scheduler.py --once
```

### Database Restore

**File**: [`v2/backend/scripts/restore_database.py`](v2/backend/scripts/restore_database.py)

Restore from backup:

```bash
# Restore latest backup
python scripts/restore_database.py --latest

# Restore specific file
python scripts/restore_database.py --file backup_20251112.sql.gz

# List available backups
python scripts/restore_database.py --list

# Skip confirmation
python scripts/restore_database.py --latest --force
```

---

## üìä Analytics System

### Real-Time Event Tracking

**Files**:
- [`v2/backend/services/analytics/analytics_engine.py`](v2/backend/services/analytics/analytics_engine.py)
- [`v2/backend/core/analytics_middleware.py`](v2/backend/core/analytics_middleware.py)

**What's Tracked**:
- ‚úÖ User registrations and logins
- ‚úÖ API requests (all endpoints)
- ‚úÖ AI queries and token usage
- ‚úÖ Code executions
- ‚úÖ File operations
- ‚úÖ Errors and performance
- ‚úÖ Feature usage

**Automatic Tracking**:
All API requests are automatically tracked by the analytics middleware. No code changes needed!

### Analytics API Endpoints

**File**: [`v2/backend/api/analytics.py`](v2/backend/api/analytics.py)

Available endpoints:

```bash
# Dashboard metrics
GET /api/v2/analytics/dashboard?days=30

# User activity
GET /api/v2/analytics/users?limit=100

# AI usage by model
GET /api/v2/analytics/ai?days=7

# API performance metrics
GET /api/v2/analytics/performance?days=7

# Error summary
GET /api/v2/analytics/errors?days=7

# Real-time stats
GET /api/v2/analytics/realtime/stats

# Export data
GET /api/v2/analytics/export?format=json&days=30
GET /api/v2/analytics/export?format=csv&table=ai_usage
```

### Database Schema

**File**: [`v2/backend/migrations/003_analytics.sql`](v2/backend/migrations/003_analytics.sql)

8 tables created:
1. `analytics.events` - Raw event stream
2. `analytics.user_sessions` - Session tracking
3. `analytics.ai_usage` - AI model usage and costs
4. `analytics.api_performance` - Endpoint performance
5. `analytics.usage_metrics` - Aggregated daily/hourly stats
6. `analytics.feature_usage` - Feature usage tracking
7. `analytics.errors` - Error tracking
8. `analytics.system_health` - Infrastructure monitoring

---

## üè• Health Check System

**File**: [`v2/backend/core/health_checks.py`](v2/backend/core/health_checks.py)

Comprehensive validation:
- Database connectivity
- Redis connectivity
- AI provider (OpenRouter)
- Disk space
- Memory usage
- Python imports
- Database extensions
- Table existence

**Usage**:
```bash
# Simple health check
curl http://localhost:8000/health

# Detailed diagnostics
curl http://localhost:8000/health/detailed
```

---

## üìà Monitoring (Prometheus + Grafana)

### Prometheus Configuration

**File**: [`v2/backend/monitoring/prometheus.yml`](v2/backend/monitoring/prometheus.yml)

Metrics exposed at: `http://localhost:8000/metrics`

**Tracked Metrics**:
- `http_requests_total` - Total HTTP requests by endpoint
- `http_request_duration_seconds` - Response time distribution
- `ai_requests_total` - AI queries by model
- `ai_tokens_total` - Token usage by model

### Alert Rules

**File**: [`v2/backend/monitoring/alerts/nexuslang_alerts.yml`](v2/backend/monitoring/alerts/nexuslang_alerts.yml)

Alerts configured for:
- High error rate (> 5%)
- Slow API responses (> 2s)
- Database down
- Redis down
- High memory usage (> 90%)
- Low disk space (< 10%)
- AI provider errors
- Excessive AI costs

### Grafana Dashboards

**File**: [`v2/backend/monitoring/grafana/dashboards/nexuslang_overview.json`](v2/backend/monitoring/grafana/dashboards/nexuslang_overview.json)

Visualizes:
- Total users
- Active users
- API request rate
- Error rate
- Response times
- AI usage by model
- Database connections
- System resources

---

## üíæ Data Persistence

### Volume Configuration

**File**: [`configure_volumes.sh`](configure_volumes.sh)

Ensures these directories persist on network volume:
- PostgreSQL data ‚Üí `/workspace/postgresql-data`
- Redis data ‚Üí `/workspace/redis-data`
- Application logs ‚Üí `/workspace/logs`
- Database backups ‚Üí `/workspace/backups`
- Pip cache ‚Üí `/workspace/pip-cache`

**Usage**:
```bash
# Run once on pod setup
bash configure_volumes.sh
```

After this, pod restarts will preserve all data!

---

## üß™ Deployment Validation

### Automated Testing

**Files**:
- [`v2/backend/scripts/validate_deployment.py`](v2/backend/scripts/validate_deployment.py)
- [`v2/backend/tests/test_deployment.py`](v2/backend/tests/test_deployment.py)

**Quick Validation**:
```bash
# Run validation script
cd v2/backend
python scripts/validate_deployment.py

# Run pytest tests
pytest tests/test_deployment.py -v
```

**What's Tested**:
- ‚úÖ Health endpoints
- ‚úÖ Database connectivity
- ‚úÖ Redis connectivity
- ‚úÖ Authentication flow
- ‚úÖ Analytics system
- ‚úÖ API documentation
- ‚úÖ All critical endpoints

---

## üìñ Common Scenarios

### Scenario 1: Fresh Pod Deployment

```bash
# First time deployment
cd /workspace
git clone https://github.com/galion-studio/nexuslang-v2.git
cd nexuslang-v2

# Run recovery script
python recovery.py

# Your platform is now live!
```

### Scenario 2: Pod Restarted (Data Preserved)

```bash
# After pod restart (with network volume configured)
cd /workspace/nexuslang-v2

# Quick recovery (database already exists)
python recovery.py

# Done in < 2 minutes!
```

### Scenario 3: Manual Deployment (No Docker)

```bash
cd /workspace/nexuslang-v2

# Force manual installation
python recovery.py --manual
```

### Scenario 4: Fresh Start (No Database Restore)

```bash
# Skip backup restoration for fresh database
python recovery.py --skip-backup
```

### Scenario 5: Daily Database Backup

```bash
# Add to crontab or run manually
python v2/backend/scripts/backup_database.py --upload-s3
```

---

## üîç Monitoring & Debugging

### View Logs

```bash
# Backend logs
tail -f /tmp/nexus.log

# Analytics events (last 100)
psql -U nexus -d nexus_v2 -c "SELECT * FROM analytics.events ORDER BY timestamp DESC LIMIT 100;"
```

### Check Service Status

```bash
# Check if server running
ps aux | grep python

# Check PostgreSQL
service postgresql status

# Check Redis
redis-cli ping

# Full health check
curl http://localhost:8000/health/detailed | jq
```

### View Analytics

```bash
# Dashboard metrics
curl -H "Authorization: Bearer YOUR_TOKEN" \
  http://localhost:8000/api/v2/analytics/dashboard | jq

# AI usage
curl -H "Authorization: Bearer YOUR_TOKEN" \
  http://localhost:8000/api/v2/analytics/ai | jq
```

---

## üìä Analytics Dashboard Example Queries

### Get Active Users

```sql
SELECT COUNT(DISTINCT user_id) 
FROM analytics.events 
WHERE timestamp > NOW() - INTERVAL '24 hours';
```

### AI Usage by Model

```sql
SELECT 
    model,
    COUNT(*) as queries,
    SUM(total_tokens) as tokens,
    SUM(estimated_cost_credits) as cost
FROM analytics.ai_usage
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY model
ORDER BY queries DESC;
```

### Top Endpoints by Usage

```sql
SELECT 
    endpoint,
    COUNT(*) as calls,
    AVG(response_time_ms) as avg_time_ms
FROM analytics.api_performance
WHERE timestamp > NOW() - INTERVAL '24 hours'
GROUP BY endpoint
ORDER BY calls DESC
LIMIT 10;
```

### Error Rate Trend

```sql
SELECT 
    DATE(timestamp) as date,
    SUM(CASE WHEN success = false THEN 1 ELSE 0 END)::FLOAT / COUNT(*) as error_rate
FROM analytics.api_performance
GROUP BY DATE(timestamp)
ORDER BY date DESC;
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

Key variables for analytics and monitoring:

```env
# Analytics (automatically configured)
DATABASE_URL=postgresql+asyncpg://nexus:pass@localhost:5432/nexus_v2

# S3 Backup (optional)
S3_ENDPOINT=https://s3.amazonaws.com
S3_BUCKET=nexuslang-backups
S3_ACCESS_KEY=your-access-key
S3_SECRET_KEY=your-secret-key

# Monitoring (optional)
SENTRY_DSN=https://your-sentry-dsn
```

### Adding Analytics Middleware

Already integrated in [`v2/backend/main.py`](v2/backend/main.py)!

The analytics middleware automatically tracks all requests.

---

## üéä Benefits Summary

### Before This System:
- ‚ùå 20+ minutes to recover from pod restart
- ‚ùå Manual database recreation
- ‚ùå Lost all user data
- ‚ùå No analytics or monitoring
- ‚ùå Manual health checks

### After This System:
- ‚úÖ < 2 minutes recovery time
- ‚úÖ Automatic database backup/restore
- ‚úÖ All data preserved on network volume
- ‚úÖ Comprehensive analytics tracking every action
- ‚úÖ Real-time monitoring dashboards
- ‚úÖ Automated health validation
- ‚úÖ One-command deployment

---

## üìû Support

If you encounter issues:

1. **Check logs**: `tail -f /tmp/nexus.log`
2. **Run health check**: `curl http://localhost:8000/health/detailed`
3. **Validate deployment**: `python scripts/validate_deployment.py`
4. **Review analytics**: Check `/api/v2/analytics/dashboard`

---

## üéØ Next Steps

1. **Run initial deployment**: `python recovery.py`
2. **Configure volumes**: `bash configure_volumes.sh`
3. **Schedule backups**: `python scripts/backup_scheduler.py &`
4. **Access analytics**: Visit `/api/v2/analytics/dashboard`
5. **Monitor metrics**: Set up Grafana with provided dashboards

---

**üéâ Your NexusLang v2 platform now has enterprise-grade recovery and analytics!**

